{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying the TF-IDF transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we apply the TF-IDF transformation, it is obligatory to put aside some\n",
    "test data for evaluating our model later. Otherwise, a future Machine Learning model would have access to statistics of the\n",
    "entire dataset and may deduce statistics of the test dataset afterwards.\n",
    "However, the entire purpose of the train-test-split is to evaluate the model on\n",
    "data it has not seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"../data/processed/data.json\")\n",
    "df = df.loc[df[\"Procedures_Length\"] > 0, [\n",
    "    \"Label\", \n",
    "    \"Procedures\", \n",
    "    \"Description\", \n",
    "    \"Procedures_Length\", \n",
    "    \"Description_Length\",\n",
    "    \"Procedures_Description_Ratio\"\n",
    "]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a train-test-split\n",
    "\n",
    "With `sklearn`, splitting a `DataFrame` reduces to calling the `train_test_split`\n",
    "function from the `model_selection` module. The `test_size` argument determines\n",
    "the relative size of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = df.drop(columns=[\"Label\"]), df[\"Label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we split up our target column `Label` from the rest so that it will\n",
    "not be included in the following transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting `TfidfVectorizer`s\n",
    "\n",
    "Since we have two text columns (`Procedures` and `Description`), it is best to\n",
    "fit two `TfidfVectorizer`s so that all information contained in those two\n",
    "separately will be preserved.\n",
    "The rest of the features should be _scaled_ as certain models encounter\n",
    "numerical problems when two features are on very different scales (that is to\n",
    "say one feature usually is very large, e.g. $\\gg 10^6$, while another only attains\n",
    "values between 0 and 1). To do all of this in one go, `sklearn` provides us with\n",
    "a [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) that takes a list of tuples consisting of a column name\n",
    "and a transformer that should transform the corresponding column. Additionally,\n",
    "the `ColumnTransformer`'s `remainder` keyword argument may be another\n",
    "transformer that will be applied to the remaining columns. Here's how to use it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "columnwise_tfidf = ColumnTransformer(\n",
    "    [\n",
    "        (\n",
    "            \"procedures\", \n",
    "            TfidfVectorizer(), \n",
    "            \"Procedures\"\n",
    "        ),\n",
    "        (\n",
    "            \"desc\", \n",
    "            TfidfVectorizer(), \n",
    "            \"Description\"\n",
    "        )\n",
    "    ],\n",
    "    remainder=StandardScaler(),\n",
    "    n_jobs=-1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the first item in the tuple is a name for the transformation for later\n",
    "reference. Second, the [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html?highlight=tfidfvectorizer#sklearn.feature_extraction.text.TfidfVectorizer) with standard arguments constructs the\n",
    "TF-IDF vectors in almost the same way that I explained it in the Blog Post accompanying this part of the project. The only\n",
    "difference is that the document frequency of each word is increased by one to\n",
    "prevent zero divisions. Third and last, the [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) scales the\n",
    "remaining features such that they have zero mean and unit standard deviation.\n",
    "\n",
    "Applying this `ColumnTransformer` to our train set follows the usual `sklearn`\n",
    "API. Each `Transformer` has `fit` and `transform` methods. Here, the first is\n",
    "used /solely on the train set/ to fit the `Transformer`. Afterwards, the second\n",
    "may be used to transform both the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnwise_tfidf.fit(X_train)\n",
    "X_train_transformed = columnwise_tfidf.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conveniently, most transformers have a `fit_transform` method that combines\n",
    "these two steps into one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = columnwise_tfidf.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use the fitted transformers to extract keywords from articles. First, we will extract the vocabulary as determined by the `TfidfVectorizer`s. To distinguish between the words from the Procedures and the Description, we will prepend each of them with a prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocabulary():\n",
    "    return (\n",
    "        [f\"proc__{name}\" for name in columnwise_tfidf.named_transformers_[\"procedures\"].get_feature_names()]\n",
    "        + [f\"desc__{name}\" for name in columnwise_tfidf.named_transformers_[\"desc\"].get_feature_names()]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the names we have provided for the `TfidfVectorizer`s earlier now come into play.\n",
    "\n",
    "Second, let's write a function accepting an article and returning a `DataFrame` containing the words with the highest frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(article, topn=10):\n",
    "    article_transformed = columnwise_tfidf.transform(article).toarray()[0]\n",
    "    frequencies = list(zip(vocabulary(), article_transformed))\n",
    "    frequencies.sort(key=lambda x: -x[1])\n",
    "    return pd.DataFrame(frequencies[:topn])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's extract keywords from one of the most iconic SCP articles: The one for [SCP-682](http://www.scp-wiki.net/scp-682). This is one of the best examples of Keter class SCPs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>proc__682</td>\n",
       "      <td>0.767357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>desc__kia</td>\n",
       "      <td>0.738121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>desc__682</td>\n",
       "      <td>0.523255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>desc__agent</td>\n",
       "      <td>0.171312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>desc__personnel</td>\n",
       "      <td>0.156161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>proc__speak</td>\n",
       "      <td>0.153737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>proc__acid</td>\n",
       "      <td>0.144138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>proc__to</td>\n",
       "      <td>0.133515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>desc__pvt</td>\n",
       "      <td>0.110179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>proc__scp</td>\n",
       "      <td>0.107281</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0         1\n",
       "0        proc__682  0.767357\n",
       "1        desc__kia  0.738121\n",
       "2        desc__682  0.523255\n",
       "3      desc__agent  0.171312\n",
       "4  desc__personnel  0.156161\n",
       "5      proc__speak  0.153737\n",
       "6       proc__acid  0.144138\n",
       "7         proc__to  0.133515\n",
       "8        desc__pvt  0.110179\n",
       "9        proc__scp  0.107281"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scp_682 = df.loc[df[\"Description\"].str.startswith(\"SCP-682\")].drop(columns=[\"Label\"])\n",
    "extract_keywords(scp_682)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not look too promising. First, maybe numbers should be ignored. Then, there are words \"to\", \"of\" appearing in almost every article in english. \"speak\" might also not be telling much. This will only get worse if we look at the top 30 keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>proc__682</td>\n",
       "      <td>0.767357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>desc__kia</td>\n",
       "      <td>0.738121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>desc__682</td>\n",
       "      <td>0.523255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>desc__agent</td>\n",
       "      <td>0.171312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>desc__personnel</td>\n",
       "      <td>0.156161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>proc__speak</td>\n",
       "      <td>0.153737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>proc__acid</td>\n",
       "      <td>0.144138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>proc__to</td>\n",
       "      <td>0.133515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>desc__pvt</td>\n",
       "      <td>0.110179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>proc__scp</td>\n",
       "      <td>0.107281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>desc__handled</td>\n",
       "      <td>0.106319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>proc__attempts</td>\n",
       "      <td>0.098297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>proc__reacted</td>\n",
       "      <td>0.095920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>desc__occurrence</td>\n",
       "      <td>0.095232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>proc__incapacitation</td>\n",
       "      <td>0.091120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>proc__of</td>\n",
       "      <td>0.090828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>proc__fear</td>\n",
       "      <td>0.087715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>proc__rage</td>\n",
       "      <td>0.087715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>proc__hydrochloric</td>\n",
       "      <td>0.085073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>proc__massive</td>\n",
       "      <td>0.085073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>proc__frequent</td>\n",
       "      <td>0.082915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>proc__provoking</td>\n",
       "      <td>0.082915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>proc__breach</td>\n",
       "      <td>0.082463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>desc__scp</td>\n",
       "      <td>0.081648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>proc__should</td>\n",
       "      <td>0.080923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>proc__lining</td>\n",
       "      <td>0.079510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>proc__called</td>\n",
       "      <td>0.078116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>proc__incapacitated</td>\n",
       "      <td>0.078116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>proc__force</td>\n",
       "      <td>0.078011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>proc__destroying</td>\n",
       "      <td>0.076869</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0         1\n",
       "0              proc__682  0.767357\n",
       "1              desc__kia  0.738121\n",
       "2              desc__682  0.523255\n",
       "3            desc__agent  0.171312\n",
       "4        desc__personnel  0.156161\n",
       "5            proc__speak  0.153737\n",
       "6             proc__acid  0.144138\n",
       "7               proc__to  0.133515\n",
       "8              desc__pvt  0.110179\n",
       "9              proc__scp  0.107281\n",
       "10         desc__handled  0.106319\n",
       "11        proc__attempts  0.098297\n",
       "12         proc__reacted  0.095920\n",
       "13      desc__occurrence  0.095232\n",
       "14  proc__incapacitation  0.091120\n",
       "15              proc__of  0.090828\n",
       "16            proc__fear  0.087715\n",
       "17            proc__rage  0.087715\n",
       "18    proc__hydrochloric  0.085073\n",
       "19         proc__massive  0.085073\n",
       "20        proc__frequent  0.082915\n",
       "21       proc__provoking  0.082915\n",
       "22          proc__breach  0.082463\n",
       "23             desc__scp  0.081648\n",
       "24          proc__should  0.080923\n",
       "25          proc__lining  0.079510\n",
       "26          proc__called  0.078116\n",
       "27   proc__incapacitated  0.078116\n",
       "28           proc__force  0.078011\n",
       "29      proc__destroying  0.076869"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_keywords(scp_682, topn=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning the `TfidfVectorizer`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, `TfidfVectorizer` has a lot of options to fine-tune its behavior. First and maybe most importantly, we can enforce that certain words should be ignored via the `stop_words` keyword argument. It either expects the string \"english\" and then uses a list constructed by the `sklearn` developers (with its own set of disadvantages) or it expects a list of strings containing the words that shall be ignored. Second, we can specify a regex pattern via the `token_pattern` keyword argument. This pattern will be used when parsing the articles to build up the vocabulary. The standard pattern includes single words containing letters and numbers; we will modify it to only parse for words containing letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ColumnTransformer(n_jobs=None,\n",
       "                  remainder=StandardScaler(copy=True, with_mean=True,\n",
       "                                           with_std=True),\n",
       "                  sparse_threshold=0.3, transformer_weights=None,\n",
       "                  transformers=[('procedures',\n",
       "                                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                                 decode_error='strict',\n",
       "                                                 dtype=<class 'numpy.float64'>,\n",
       "                                                 encoding='utf-8',\n",
       "                                                 input='content',\n",
       "                                                 lowercase=True, max_df=1.0,\n",
       "                                                 max_features=None, min_df=1...\n",
       "                                                 dtype=<class 'numpy.float64'>,\n",
       "                                                 encoding='utf-8',\n",
       "                                                 input='content',\n",
       "                                                 lowercase=True, max_df=1.0,\n",
       "                                                 max_features=None, min_df=1,\n",
       "                                                 ngram_range=(1, 1), norm='l2',\n",
       "                                                 preprocessor=None,\n",
       "                                                 smooth_idf=True,\n",
       "                                                 stop_words='english',\n",
       "                                                 strip_accents='unicode',\n",
       "                                                 sublinear_tf=False,\n",
       "                                                 token_pattern='(?u)\\\\b[a-zA-Z][a-zA-Z]+\\\\b',\n",
       "                                                 tokenizer=None, use_idf=True,\n",
       "                                                 vocabulary=None),\n",
       "                                 'Description')],\n",
       "                  verbose=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columnwise_tfidf = ColumnTransformer(\n",
    "    [\n",
    "        (\n",
    "            \"procedures\", \n",
    "            TfidfVectorizer(\n",
    "                stop_words=\"english\", \n",
    "                strip_accents='unicode',\n",
    "                token_pattern='(?u)\\\\b[a-zA-Z][a-zA-Z]+\\\\b',\n",
    "            ), \n",
    "            \"Procedures\"\n",
    "        ),\n",
    "        (\n",
    "            \"desc\", \n",
    "            TfidfVectorizer(\n",
    "                stop_words=\"english\", \n",
    "                strip_accents='unicode',\n",
    "                token_pattern='(?u)\\\\b[a-zA-Z][a-zA-Z]+\\\\b'\n",
    "            ), \n",
    "            \"Description\"\n",
    "        )\n",
    "    ],\n",
    "    remainder=StandardScaler()\n",
    ")\n",
    "\n",
    "columnwise_tfidf.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>desc__kia</td>\n",
       "      <td>0.890278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>proc__speak</td>\n",
       "      <td>0.272335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>proc__acid</td>\n",
       "      <td>0.255331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>desc__agent</td>\n",
       "      <td>0.206627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>proc__scp</td>\n",
       "      <td>0.190041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>desc__personnel</td>\n",
       "      <td>0.188352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>proc__attempts</td>\n",
       "      <td>0.174127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>proc__reacted</td>\n",
       "      <td>0.169915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>proc__incapacitation</td>\n",
       "      <td>0.161413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>proc__fear</td>\n",
       "      <td>0.155381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>proc__rage</td>\n",
       "      <td>0.155381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>proc__hydrochloric</td>\n",
       "      <td>0.150702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>proc__massive</td>\n",
       "      <td>0.150702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>proc__frequent</td>\n",
       "      <td>0.146879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>proc__provoking</td>\n",
       "      <td>0.146879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>proc__breach</td>\n",
       "      <td>0.146078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>proc__lining</td>\n",
       "      <td>0.140847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>proc__called</td>\n",
       "      <td>0.138377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>proc__incapacitated</td>\n",
       "      <td>0.138377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>proc__force</td>\n",
       "      <td>0.138192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>proc__destroying</td>\n",
       "      <td>0.136168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>proc__containment</td>\n",
       "      <td>0.135959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>desc__pvt</td>\n",
       "      <td>0.132891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>proc__difficulty</td>\n",
       "      <td>0.132345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>proc__submerged</td>\n",
       "      <td>0.132345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>proc__best</td>\n",
       "      <td>0.130666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>desc__handled</td>\n",
       "      <td>0.128236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>proc__chamber</td>\n",
       "      <td>0.126861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>proc__plate</td>\n",
       "      <td>0.125041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>proc__development</td>\n",
       "      <td>0.123843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0         1\n",
       "0              desc__kia  0.890278\n",
       "1            proc__speak  0.272335\n",
       "2             proc__acid  0.255331\n",
       "3            desc__agent  0.206627\n",
       "4              proc__scp  0.190041\n",
       "5        desc__personnel  0.188352\n",
       "6         proc__attempts  0.174127\n",
       "7          proc__reacted  0.169915\n",
       "8   proc__incapacitation  0.161413\n",
       "9             proc__fear  0.155381\n",
       "10            proc__rage  0.155381\n",
       "11    proc__hydrochloric  0.150702\n",
       "12         proc__massive  0.150702\n",
       "13        proc__frequent  0.146879\n",
       "14       proc__provoking  0.146879\n",
       "15          proc__breach  0.146078\n",
       "16          proc__lining  0.140847\n",
       "17          proc__called  0.138377\n",
       "18   proc__incapacitated  0.138377\n",
       "19           proc__force  0.138192\n",
       "20      proc__destroying  0.136168\n",
       "21     proc__containment  0.135959\n",
       "22             desc__pvt  0.132891\n",
       "23      proc__difficulty  0.132345\n",
       "24       proc__submerged  0.132345\n",
       "25            proc__best  0.130666\n",
       "26         desc__handled  0.128236\n",
       "27         proc__chamber  0.126861\n",
       "28           proc__plate  0.125041\n",
       "29     proc__development  0.123843"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_keywords(scp_682, topn=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much better. A few remarks:\n",
    "\n",
    "- I had to google for the two abbreviations \"kia\" and \"pvt\". The first is the abbreviation for \"killed in action\" while the second stands for the military rank \"Private\".\n",
    "- On second thought, \"speak\" *may* contain the information that the SCP object is able to speak and, thusly, might hint at it being sapient. As sapient SCPs are probably more likely to be of class euclid or keter, this could be valuable information for a Machine Learning model.\n",
    "- One could start building a custom list of stop words more suitable for parsing SCP articles. In the list above, the words \"best\" and \"called\" as well as \"scp\" could be ignored. I will postpone this to the next part of this series of posts. Because some models give some insight in their learning process, we can use them to see if their decisions are based on filler words."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
